<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 17px;
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="files/JHU_icon.jpg">
  <title>Shao-Yuan Lo</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Shao-Yuan Lo &nbsp 羅紹元</name>
              </p>
              <p align="justify">
				I am a Research Scientist at <a href="https://usa.honda-ri.com/"> Honda Research Institute USA</a>.
				My research lies in Machine Learning, Computer Vision, and Trustworthy AI. 
				Specifically, my recent research focuses on Multimodal Large Language Models, Visual Scene Understanding, and Adversarial Machine Learning.			
              </p>
              <p align="justify">
				I obtained my Ph.D. in Electrical and Computer Engineering at 
				<a href="https://www.jhu.edu/">Johns Hopkins University</a> in 2023, advised by Prof. 
				<a href="https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/">Vishal M. Patel</a>.
				Before that, I received my M.S. (Electronics Engineering) and B.S. (EECS Honors Program) degrees from 
				<a href="https://en.nycu.edu.tw/">National Chiao Tung University</a> in 2019 and 2017, respectively, advised by Prof. 
				<a href="https://mcube.nctu.edu.tw/~hmhang/">Hsueh-Ming Hang</a>.
                I also spent wonderful times as an Applied Scientist Intern at
				<a href="https://justwalkout.com/"> Amazon Just Walk Out team</a> in summer 2022 and at 
				<a href="https://www.youtube.com/watch?v=sj1t3msy8dc">Amazon Astro team</a> in summer 2021.
              </p>
              <p align=center>
                <a href="mailto:shao-yuan_lo@honda-ri.com">Email</a> &nbsp/&nbsp
                <a href="files/SYLo_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=vL35nH4AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/shaoyuanlo"> Github </a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shaoyuanlo/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://twitter.com/shaoyuanlo">Twitter</a>
              </p>
			  <!-- <p align=justify>
                <strong>Hiring Summer 2024 Research Interns!!</strong>
				I am always open to collaborating with self-motivated graduate students to conduct advanced research in industry. 
				If you are interested in working with me, please drop me an email along with your resume.
              </p> -->
			  <!-- <p align=center>
                <h3 style="color:red;text-align:center">
				I expected to graduate in Spring 2023. I am currently on the job market and looking for Research Scientist roles in the industry.
				</h3>
              </p> -->
            </td>
            <td width="33%">
              <img src="files/sylo3.jpg" height="264" width="220">
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>News</heading>
              <ul>
                <li>02/2024: &nbsp 2 papers accepted to CVPR 2024. Won the Best Student Paper Award at SPIE Medical Imaging 2024!!</li>
              </ul>			  
              <ul>
                <li>01/2024: &nbsp Invited talks at Academia Sinica and National Yang Ming Chiao Tung University.</li>
              </ul>			  
			  <ul>
                <li>07/2023: &nbsp Joined Honda Research Institute USA as a Research Scientist. See you in San Jose, CA!!</li>
              </ul>
              <ul>
                <li>03/2023: &nbsp Defended my PhD dissertation!!</li>
              </ul>			  			  
              <ul>
                <li>02/2023: &nbsp 1 paper accepted to CVPR 2023.</li>
              </ul>			  
			  <ul>
                <li>09/2022: &nbsp Accepted to the Google CS Research Mentorship Program.</li>
              </ul>	
              <ul>
                <li>07/2022: &nbsp 1 paper accepted to IEEE T-PAMI, and 1 paper accepted to IROS 2022.</li>
              </ul>
			  <ul>
                <li>05/2022: &nbsp Joined Amazon (Just Walk Out team) as an Applied Scientist Intern. See you in Seattle, WA!!</li>
              </ul>			  
              <ul>
                <li>01/2022: &nbsp Invited talks at Academia Sinica and National Yang Ming Chiao Tung University.</li>
              </ul>			  
              <ul>
                <li>12/2021: &nbsp 1 paper accepted to IEEE T-IP.</li>
			  </ul>	
              <ul>
                <li>06/2021: &nbsp Invited talk at CVPR 2021 Tutorial on Adversarial Machine Learning in Computer Vision.</li>
              </ul>					  
			  <ul>
                <li>05/2021: &nbsp Joined Amazon (Astro team) as an Applied Scientist Intern. See you in Bellevue, WA!!</li>
              </ul>			  
              <ul>
                <li>12/2019: &nbsp Won the Best Paper Award at ACM Multimedia Asia 2019!!</li>
              </ul>
			  <ul>
                <li>08/2019: &nbsp Joined Johns Hopkins University as a PhD student. See you in Baltimore, MD!!</li>
              </ul>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="25%" valign="middle">
              <heading>Publications</heading>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="papers/UADT.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Uncertainty-aware Action Decoupling Transformer for Action Anticipation</papertitle>
                <br>
				Hongji Guo, &nbsp Nakul Agarwal, &nbsp <strong>Shao-Yuan Lo</strong>, &nbsp Kwonjoon Lee, &nbsp Qiang Ji
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
				<br>
				<strong>[Highlight]</strong>
                <br>
				<br>
                <a href="papers/UADT.bib">bibtex</a>
              </p>
            </td>
          </tr>	

          <tr>
            <td width="25%"><img src="papers/PlausiVL.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Can’t Make an Omelette without Breaking Some Eggs: Plausible Action Anticipation using Large Video-Language Models</papertitle>
                <br>
				Himangi Mittal, &nbsp Nakul Agarwal, &nbsp <strong>Shao-Yuan Lo</strong>, &nbsp Kwonjoon Lee
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
                <br>
				<br>
                <a href="papers/PlausiVL.bib">bibtex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="papers/X-CBCT-DDPM.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Patient-specific 3D Volumetric CBCT Image Reconstruction with Single X-ray Projection Using Denoising Diffusion Probabilistic Model</papertitle>
                <br>
                Shaoyan Pan, &nbsp <strong>Shao-Yuan Lo</strong>, &nbsp Chih-Wei Chang, &nbsp Ella Salari, &nbsp Tonghe Wang, &nbsp Justin Roper,
                &nbsp Aparna H. Kesarwala, &nbsp Xiaofeng Yang
                <br>
                <em>SPIE Medical Imaging (<strong>SPIE MI</strong>)</em>, 2024
                <br>
				<font color="red">[Best Student Paper Award]</font>
                <br>
				<br>
                <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12931/129310P/Patient-specific-3D-volumetric-CBCT-image-reconstruction-with-single-x/10.1117/12.3006561.short">paper</a> / 
                <a href="papers/X-CBCT-DDPM.bib">bibtex</a>
              </p>
            </td>
          </tr>	

		  <tr>
            <td width="25%"><img src="papers/ABNN.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Adaptive Batch Normalization Networks for Adversarial Robustness</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Vishal M. Patel
                <br>
                <em>IEEE International Conference on Advanced Video and Signal-based Surveillance (<strong>AVSS</strong>)</em>, 2024
                <br>
				<br>
                <a href="https://arxiv.org/abs/2405.11708">paper</a> / 
                <a href="papers/ABNN.bib">bibtex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="papers/STPL.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation</papertitle>
                <br>
				<strong>Shao-Yuan Lo</strong>, &nbsp Poojan Oza, &nbsp Sumanth Chennupati, &nbsp Alejandro Galindo, &nbsp Vishal M. Patel
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
                <br>
				<br>
                <a href="https://arxiv.org/abs/2303.14361">paper</a> / 
				<a href="papers/STPL.pdf">slides</a> / 
				<a href="papers/STPL_poster.pdf">poster</a> / 
                <a href="papers/STPL.bib">bibtex</a>
              </p>
            </td>
          </tr>	

          <tr>
            <td width="25%"><img src="papers/AFAMI.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Deep Learning-based Multi-Organ CT Segmentation with Adversarial Data Augmentation</papertitle>
                <br>
                Shaoyan Pan, &nbsp <strong>Shao-Yuan Lo</strong>, &nbsp Min Huang, &nbsp Chaoqiong Ma, &nbsp Jacob Wynne, &nbsp Tonghe Wang,
                &nbsp Tian Liu, &nbsp Xiaofeng Yang
                <br>
                <em>SPIE Medical Imaging (<strong>SPIE MI</strong>)</em>, 2023
                <br>
				<br>
                <a href="https://arxiv.org/abs/2302.13172">paper</a> / 
                <a href="papers/AFAMI.bib">bibtex</a>
              </p>
            </td>
          </tr>	

          <tr>
            <td width="25%"><img src="papers/PrincipaLS.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Adversarially Robust One-class Novelty Detection</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Poojan Oza, &nbsp Vishal M. Patel
                <br>
                <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>)</em>, 2022
                <br>
				<br>
                <a href="https://arxiv.org/abs/2108.11168">paper</a> / 
                <a href="papers/PrincipaLS.bib">bibtex</a>
              </p>
            </td>
          </tr>	
		  
		  <tr>
            <td width="25%"><img src="papers/LFDA.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Learning Feature Decomposition for Domain Adaptive Monocular Depth Estimation</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Wei Wang, &nbsp Jim Thomas, &nbsp Jingjing Zheng, &nbsp Vishal M. Patel, &nbsp Cheng-Hao Kuo
                <br>
                <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2022
                <br>
				<br>
                <a href="https://arxiv.org/abs/2208.00160">paper</a> / 
                <a href="https://www.amazon.science/blog/transferring-depth-estimation-knowledge-between-cameras">blog</a> / 
                <a href="papers/PLS.bib">bibtex</a>
              </p>
            </td>
          </tr>	
		  
		  <tr>
            <td width="25%"><img src="papers/ARTUDA.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Exploring Adversarially Robust Training for Unsupervised Domain Adaptation</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Vishal M. Patel
                <br>
                <em>Asian Conference on Computer Vision (<strong>ACCV</strong>)</em>, 2022
                <br>
				<br>
                <a href="https://arxiv.org/abs/2202.09300">paper</a> / 
				<a href="papers/ARTUDA.pdf">slides</a> / 
                <a href="papers/ARTUDA.bib">bibtex</a>
              </p>
            </td>
          </tr>	
		  
		  <tr>
            <td width="25%"><img src="papers/MultiBN.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Defending Against Multiple and Unforeseen Adversarial Videos</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Vishal M. Patel
                <br>
                <em>IEEE Transactions on Image Processing (<strong>T-IP</strong>)</em>, 2021
                <br>
				<strong>[Journal presentation at ICIP 2022]</strong>
                <br>
				<br>
                <a href="https://arxiv.org/abs/2009.05244">paper</a> / 
				<a href="papers/MultiBN.pdf">slides</a> / 
                <a href="papers/MultiBN.bib">bibtex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="papers/Halftone.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Error Diffusion Halftoning Against Adversarial Examples</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Vishal M. Patel
                <br>
                <em>IEEE International Conference on Image Processing (<strong>ICIP</strong>)</em>, 2021
                <br>
				<br>
                <a href="https://arxiv.org/abs/2101.09451">paper</a> / 
				<a href="papers/Halftone.pdf">slides</a> / 
                <a href="papers/Halftone.bib">bibtex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="papers/OUDefend.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Overcomplete Representations Against Adversarial Videos</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Jeya Maria Jose Valanarasu, &nbsp Vishal M. Patel
                <br>
                <em>IEEE International Conference on Image Processing (<strong>ICIP</strong>)</em>, 2021
                <br>
				<br>
                <a href="https://arxiv.org/abs/2012.04262">paper</a> / 
				<a href="papers/OUDefend.pdf">slides</a> / 
                <a href="papers/OUDefend.bib">bibtex</a>
              </p>
            </td>
          </tr>

		  <tr>
            <td width="25%"><img src="papers/MultAV.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>MultAV: Multiplicative Adversarial Videos</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Vishal M. Patel
                <br>
                <em>IEEE International Conference on Advanced Video and Signal-based Surveillance (<strong>AVSS</strong>)</em>, 2021
                <br>
				<br>
                <a href="https://arxiv.org/abs/2009.08058">paper</a> / 
				<a href="papers/MultAV.pdf">slides</a> / 
                <a href="papers/MultAV.bib">bibtex</a>
              </p>
            </td>
          </tr> 

          <tr>
            <td width="25%"><img src="papers/EDANet.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Hsueh-Ming Hang, &nbsp Sheng-Wei Chan, &nbsp Jing-Jhih Lin
                <br>
                <em>ACM International Conference on Multimedia in Asia (<strong>ACM MM Asia</strong>)</em>, 2019
				<br>
				<font color="red">[Best Paper Award]</font>
                <br>
				<br>
                <a href="https://arxiv.org/abs/1809.06323">paper</a> / 
                <a href="https://github.com/shaoyuanlo/EDANet">project page</a> / 
                <a href="papers/EDANet.pdf">slides</a> / 
                <a href="papers/EDANet.bib">bibtex</a>
              </p>
            </td>
          </tr>
		  
          <tr>
            <td width="25%"><img src="papers/DCT.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Exploring Semantic Segmentation on the DCT Representation</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Hsueh-Ming Hang
                <br>
                <em>ACM International Conference on Multimedia in Asia (<strong>ACM MM Asia</strong>)</em>, 2019
				<br>
				<strong>[Oral]</strong>
                <br>
				<br>
                <a href="https://arxiv.org/abs/1907.10015">paper</a> / 
                <a href="papers/DCT.pdf">slides</a> / 
                <a href="papers/DCT.bib">bibtex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="papers/MultiClass.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Multi-Class Lane Semantic Segmentation using Efficient Convolutional Networks</papertitle>
                <br>
                <strong>Shao-Yuan Lo</strong>, &nbsp Hsueh-Ming Hang, &nbsp Sheng-Wei Chan, &nbsp Jing-Jhih Lin
                <br>
                <em>IEEE International Workshop on Multimedia Signal Processing (<strong>MMSP</strong>)</em>, 2019
				<br>
				<br>
                <a href="https://arxiv.org/abs/1907.09438">paper</a> / 
                <a href="papers/MultiClass.pdf">slides</a> / 
                <a href="papers/MultiClass.bib">bibtex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="papers/LDFNet.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Incorporating Luminance, Depth and Color Information by a Fusion-based Network for Semantic Segmentation</papertitle>
                <br>
                Shang-Wei Hung, &nbsp <strong>Shao-Yuan Lo</strong>, &nbsp Hsueh-Ming Hang
                <br>
                <em>IEEE International Conference on Image Processing (<strong>ICIP</strong>)</em>, 2019
				<br>
				<strong>[Oral]</strong>
                <br>
				<br>
                <a href="https://arxiv.org/abs/1809.09077">paper</a> / 
                <a href="https://github.com/shangweihung/LDFNet">project page</a> / 
                <a href="papers/LDFNet.pdf">slides</a> / 
                <a href="papers/LDFNet.bib">bibtex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="papers/LMD.PNG" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Efficient Road Lane Marking Detection with Deep Learning</papertitle>
                <br>
                Ping-Rong Chen*, &nbsp <strong>Shao-Yuan Lo*</strong>, &nbsp Hsueh-Ming Hang, &nbsp Sheng-Wei Chan, &nbsp Jing-Jhih Lin
                <br>
                <em>IEEE International Conference on Digital Signal Processing (<strong>DSP</strong>)</em>, 2018
                <br>
				<br>
                <a href="https://arxiv.org/abs/1809.03994">paper</a> / 
                <a href="papers/LMD.pdf">slides</a> / 
                <a href="papers/LMD.bib">bibtex</a>
              </p>
            </td>
          </tr>		  
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="25%" valign="middle">
              <heading>Dissertations</heading>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="files/JHU_logo.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Robust Computer Vision Against Adversarial Examples and Domain Shifts</papertitle>
                <br>
				<strong>Shao-Yuan Lo</strong>
                <br>
                <em>Ph.D. Thesis, Johns Hopkins University</em>, 2023
                <br>
				<br>
                <a href="https://jscholarship.library.jhu.edu/items/497a35ef-81cc-44f3-b9a9-b01b182a1a96">thesis</a> / 
                <a href="papers/PhD.bib">bibtex</a>
              </p>
            </td>
          </tr>	
		  
		  <tr>
            <td width="25%"><img src="files/NCTU_logo.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              <p>
                <papertitle>Real-Time Semantic Segmentation Networks for Autonomous Driving</papertitle>
                <br>
				<strong>Shao-Yuan Lo</strong>
                <br>
                <em>Master Thesis, National Chiao Tung University</em>, 2019
                <br>
				<br>
                <a href="https://hdl.handle.net/11296/7acbjd">thesis</a> / 
                <a href="papers/Master.bib">bibtex</a>
              </p>
            </td>
          </tr>	
        </table>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Industrial Experience</heading>
                <ul> <li>
				  <strong>Research Scientist</strong> at 
				  <a href="https://usa.honda-ri.com/"> Honda Research Institute USA</a>, San Jose, CA &nbsp (July 2023 - Now)
				</li> </ul>			  
                <ul> <li>
				  <strong>Applied Scientist Intern</strong> at 
				  <a href="https://justwalkout.com/"> Amazon (Just Walk Out team)</a>, Seattle, WA &nbsp (May 2022 - Aug 2022)
				</li> </ul>
				<ul> <li>
				  <strong>Applied Scientist Intern</strong> at 
				  <a href="https://www.youtube.com/watch?v=sj1t3msy8dc"> Amazon (Astro team)</a>, Bellevue, WA &nbsp (May 2021 - Aug 2021)
				</li> </ul>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Awards</heading>
                <ul> <li>
				  <strong> <a href="https://spie.org/conferences-and-exhibitions/medical-imaging/program/conferences/awards#_=_">
				  Robert F. Wagner All-Conference Best Student Paper Award</a></strong>, &nbsp SPIE Medical Imaging 2024 (2024)
				</li> </ul>			  
				<ul> <li>
				  <strong> <a href="https://cvpr2023.thecvf.com/Conferences/2023">
				  CVPR DEI Travel Award</a></strong>, &nbsp IEEE/CVF CVPR 2023 (2023)
				</li> </ul>			  
				<ul> <li>
				  <strong> <a href="https://research.google/outreach/csrmp">
				  Google CS Research Mentorship Program</a></strong>, &nbsp Google Research (2022)
				</li> </ul>
                <ul> <li>
				  <strong> <a href="https://depart.moe.edu.tw/ED2500/cp.aspx?n=ECEA48027CA1AA4E&s=FDC3BA2C714CD75F">
				  Government Scholarship to Study Abroad</a></strong>, &nbsp Ministry of Education, Taiwan (2020)
				</li> </ul>
                <ul> <li>
				  <strong>First-Year Doctoral Fellowship</strong>, &nbsp Johns Hopkins University (2019)
				</li> </ul>
                <ul> <li>
				  <strong> <a href="https://depart.moe.edu.tw/ED2500/News.aspx?n=92DC002FDE1CC633&page=1&PageSize=20">
				  Government Scholarship to World Top 100 Universities</a></strong>, &nbsp Ministry of Education, Taiwan (2019)
				</li> </ul>				
                <ul> <li>
				  <strong> <a href="http://www.acmmmasia.org/2019/files/ACMMMASIA2019-Program.pdf">
				  Best Paper Award</a></strong>, &nbsp ACM Multimedia Asia 2019 (2019)
				</li> </ul>
                <ul> <li>
				  <strong> <a href="https://140.125.183.142/wp-content/uploads/2021/09/IPPR%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%B1%86%E5%8D%9A%E7%A2%A9%E5%A3%AB%E8%AB%96%E6%96%87%E7%8D%8E%E5%BE%97%E7%8D%8E%E8%B3%87%E6%96%99.pdf">
				  Best Master Thesis Award</a></strong>, &nbsp Chinese Image Processing and Pattern Recognition Society (2019)
				</li> </ul>
                <ul> <li>
				  <strong> <a href="https://infonews-fornthu.nctu.edu.tw/index.php?topflag=1&SuperType=2&SuperTypeNo=2&type=%A6%E6%ACF&id=20190500241&action=detail">
				  Students’ Outstanding Contribution Award</a></strong>, &nbsp National Chiao Tung University (2019)
				</li> </ul>
                <ul> <li>
				  <strong>Dean's List</strong>, &nbsp National Chiao Tung University (2017)
				</li> </ul>				
                <ul> <li>
				  <strong>Scholarship to Overseas Exchange Program</strong>, &nbsp National Chiao Tung University (2016)
				</li> </ul>			  
                <ul> <li>
				  <strong> <a href="https://tw.news.yahoo.com/%25E4%25BA%25A4%25E5%25A4%25A7%25E6%25A0%25A1%25E5%258F%258B%25E5%258B%259D%25E8%258F%25AF%25E7%25A7%2591%25E6%258A%2580%25E8%2591%25A3%25E5%25BA%25A7%25E6%258F%2590%25E4%25BE%259B%25E5%258D%2583%25E8%2590%25AC%25E7%258D%258E%25E5%25AD%25B8%25E9%2587%2591%25E7%25B5%25A6%25E5%2584%25AA%25E7%25A7%2580%25E5%25AD%25B8%25E5%25BC%259F%25E5%25A6%25B9-215319757.html">
				  WINTEK Outstanding Freshman Scholarship</a></strong>, &nbsp WINTEK Corp. and National Chiao Tung University (2013)
				</li> </ul>
            </td>
          </tr>
        </table>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Invited Talks</heading>
			    <ul> <li>
				  01/09/2024: &nbsp “<a href="talks/CCVL_2023_04_14.pdf">Robust Computer Vision Against Adversarial Examples and Domain Shifts</a>.”
				  At National Yang Ming Chiao Tung University, Taiwan.
				  Host: Prof. <a href="https://sites.google.com/g2.nctu.edu.tw/wpeng/cv">Wen-Hsiao Peng</a>.
				</li> </ul>
                <ul> <li>
				  12/14/2023: &nbsp “<a href="talks/CCVL_2023_04_14.pdf">Robust Computer Vision Against Adversarial Examples and Domain Shifts</a>.”
				  At Academia Sinica, Taiwan.
				  Host: Prof. <a href="https://www.citi.sinica.edu.tw/pages/pullpull/index_en.html">Jun-Cheng Chen</a>.
				</li> </ul>		
                <ul> <li>
				  04/14/2023: &nbsp “<a href="talks/CCVL_2023_04_14.pdf">Robust Computer Vision Against Adversarial Examples and Domain Shifts</a>.”
				  At Computational Cognition, Vision, and Learning (CCVL) Lab, Johns Hopkins University.
				  Host: Prof. <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>.
				</li> </ul>	  
                <ul> <li>
				  01/10/2022: &nbsp “<a href="talks/Sinica_2022_01_10_NCTU.pdf">Defending Against Multiple and Unforeseen Adversarial Videos</a>.” 
				  At National Yang Ming Chiao Tung University, Taiwan.
				  Host: Prof. <a href="https://sites.google.com/g2.nctu.edu.tw/wpeng/cv">Wen-Hsiao Peng</a>.
				</li> </ul>
                <ul> <li>
				  01/05/2022: &nbsp “<a href="talks/Sinica_2022_01_05.pdf">Defending Against Multiple and Unforeseen Adversarial Videos</a>.”
				  At Academia Sinica, Taiwan.
				  Host: Prof. <a href="https://www.citi.sinica.edu.tw/pages/pullpull/index_en.html">Jun-Cheng Chen</a>.
				</li> </ul>				
                <ul> <li>
				  06/19/2021: &nbsp “<a href="talks/CVPR21_tutorial.pdf">Adversarial Attacks and Defenses in Videos</a>.” At 
				  <a href="https://advmlincv.github.io/cvpr21-tutorial/">CVPR 2021 Tutorial on Adversarial Machine Learning in Computer Vision</a>, Virtual.
				  Host: Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a>.
				</li> </ul>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Academic Activities</heading>
				<ul> <li>
				  <strong>Area Chair</strong>: &nbsp CVPR 2024 AI4CG Workshop
				</li> </ul>
                <ul> <li>
				  <strong>Journal Reviewer</strong>: &nbsp IEEE T-PAMI, &nbsp IEEE T-IP, &nbsp IEEE RA-L, &nbsp IEEE T-CSVT, &nbsp IEEE T-SMC, 
				  &nbsp Pattern Recognition, &nbsp Medical Physics
				</li> </ul>
                <ul> <li>
				  <strong>Conference Reviewer</strong>: &nbsp CVPR, &nbsp ICCV, &nbsp ECCV, &nbsp ICLR,
				  &nbsp AAAI, &nbsp WACV, &nbsp ACCV, &nbsp ICIP, &nbsp AVSS
				</li> </ul>
				<ul> <li>
				  <strong>Teaching Assistant</strong>: &nbsp Deep Learning (EN.520.638), Johns Hopkins University, Spring (2021, 2022, 2023)
				</li> </ul>	
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Press Coverage</heading>
				<ul> <li>
				  10/11/2022: &nbsp I was reported on <a href="https://www.ftvnews.com.tw/news/detail/2022A11F04M1">Formosa Television (民視新聞)</a>, 
				  <a href="https://www.youtube.com/watch?v=4BCV3De5QuE">EBC News (東森新聞)</a>, and 
				  <a href="https://www.ntdtv.com.tw/b5/20221022/video/345977.html">New Tang Dynasty Television (新唐人電視台)</a>
				  when I was seving as the Vice President of the Taiwanese Student Association at Johns Hopkins University.
				</li> </ul>	
                <ul> <li>
				  06/01/2019: &nbsp I was reported on <a href="https://news.ltn.com.tw/news/life/breakingnews/2809162">Liberty Times (自由時報)</a>, 
				  <a href="https://udn.com/news/story/6928/3847789">United Daily News (聯合報)</a>, and 
				  <a href="https://ctee.com.tw/industrynews/campus/100670.html">Commercial Times (工商時報)</a>
				  when I was graduating from National Chiao Tung University.
				</li> </ul>			  
                <ul> <li>
				  07/19/2013: &nbsp I was reported on <a href="https://www.chinatimes.com/newspapers/20130719000490-260107?chdtv">China Times (中國時報)</a>
				  when I was graduating from senior high school (National Experimental High School at Hsinchu Science Park).
				</li> </ul>				
                <ul> <li>
				  06/04/2010: &nbsp I was reported on <a href="https://news.ltn.com.tw/news/local/paper/400837">Liberty Times (自由時報)</a>
				  when I was graduating from junior high school.
				</li> </ul>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td><br>
              <p align="left">
                <a href="https://www.easycounter.com/">
                <img src="https://www.easycounter.com/counter.php?ww8411ww"
                border="0" alt="stats counter"></a>
				<font size="2"> &nbsp unique visitors since Oct 2020 </font>
              </p>
            </td>		  
            <td><br>
              <p align="right">
                <font size="2">
                  Template from <a href="https://jonbarron.info/">Jon Barron</a>
                </font>
              </p>
            </td>		
          </tr>
        </table>

        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) { }
        </script>
      </td>
    </tr>
  </table>
</body>

</html>
